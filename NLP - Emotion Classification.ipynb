{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa7df5c-c6c9-4de1-8da4-cdbcff392c80",
   "metadata": {},
   "source": [
    "## NLP - Emotion Classification in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a942f38-2b6a-44fd-ae8b-fbeff467529c",
   "metadata": {},
   "source": [
    "### To develop machine learning models to classify emotions in text samples, I'll go through the following steps:\n",
    "\n",
    "- Loading and Preprocessing\n",
    "- Feature Extraction\n",
    "- Model Development\n",
    "- Model Comparison\n",
    "\n",
    "Step 1: Loading and Preprocessing\n",
    "First, load the dataset and inspect it. Then, clean the text, tokenize it, and remove stopwords.\n",
    "\n",
    "Loading the Dataset\n",
    "I will load the dataset from the provided link and check its structure. For demonstration purposes, let's assume the dataset has columns like text and emotion.\n",
    "\n",
    "Preprocessing Techniques\n",
    "Text Cleaning: Removing punctuation, numbers, and special characters.\n",
    "Tokenization: Splitting text into words (tokens).\n",
    "Stopwords Removal: Removing common words that do not contribute to the meaning .\n",
    "Lemmatization/Stemming: Reducing words to their base form \n",
    "Impact on Model Performance\n",
    "Text Cleaning ensures that noise is reduced in the text data, leading to better feature extraction.\n",
    "Tokenization converts text into a format suitable for feature extraction.\n",
    "Stopwords Removal reduces the dimensionality of the text data and focuses on meaningful words.\n",
    "Lemmatization/Stemming helps in normalizing words, reducing redundancy, and improving model accuracy.\n",
    "Step 2: Feature Extraction\n",
    "Use CountVectorizer and TfidfVectorizer to transform the text data into numerical features.\n",
    "\n",
    "CountVectorizer: Converts a collection of text documents to a matrix of token counts.\n",
    "TfidfVectorizer: Converts a collection of text documents to a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features.\n",
    "The choice of vectorizer impacts the performance as TfidfVectorizer tends to give more importance to rare words and downweights common words.\n",
    "\n",
    "Step 3: Model Development\n",
    "I will train two models:\n",
    "\n",
    "Naive Bayes: Suitable for text classification due to its simplicity and effectiveness.\n",
    "Support Vector Machine (SVM): Effective in high-dimensional spaces and commonly used for text classification.\n",
    "Step 4: Model Comparison\n",
    "I will evaluate the models using metrics such as accuracy and F1-score.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5e9223-4a4c-42dc-ab68-859fc628331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Comment', 'Emotion'], dtype='object')\n",
      "                                             Comment Emotion\n",
      "0  i seriously hate one subject to death but now ...    fear\n",
      "1                 im so full of life i feel appalled   anger\n",
      "2  i sit here to write i start to dig out my feel...    fear\n",
      "3  ive been really angry with r and i feel like a...     joy\n",
      "4  i feel suspicious if there is no one outside l...    fear\n",
      "Naive Bayes Accuracy: 0.9082491582491582\n",
      "Naive Bayes F1 Score: 0.9082319465206588\n",
      "SVM Accuracy: 0.9318181818181818\n",
      "SVM F1 Score: 0.9318506955527953\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.93      0.91       392\n",
      "        fear       0.92      0.92      0.92       416\n",
      "         joy       0.93      0.88      0.90       380\n",
      "\n",
      "    accuracy                           0.91      1188\n",
      "   macro avg       0.91      0.91      0.91      1188\n",
      "weighted avg       0.91      0.91      0.91      1188\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.93      0.93      0.93       392\n",
      "        fear       0.97      0.90      0.93       416\n",
      "         joy       0.90      0.97      0.93       380\n",
      "\n",
      "    accuracy                           0.93      1188\n",
      "   macro avg       0.93      0.93      0.93      1188\n",
      "weighted avg       0.93      0.93      0.93      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1HWczIICsMpaL8EJyu48ZvRFcXx3_pcnb\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Columns:\", df.columns)\n",
    "print(df.head())\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['clean_text'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "y = df['Emotion']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "nb_f1 = f1_score(y_test, nb_pred, average='weighted')\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_f1 = f1_score(y_test, svm_pred, average='weighted')\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes F1 Score:\", nb_f1)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"SVM F1 Score:\", svm_f1)\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, nb_pred))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, svm_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b5f38-1296-4659-aaf8-708258a93bfb",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "1 Loading and Preprocessing:\n",
    "\n",
    "- Loaded the dataset from the provided link.\n",
    "\n",
    "- Preprocessed the text by removing punctuation, numbers, and stopwords, and then lemmatizing the words.\n",
    "\n",
    "2 Feature Extraction:\n",
    "\n",
    "- Used TfidfVectorizer to convert the text into numerical features.\n",
    "\n",
    "3 Model Development:\n",
    "\n",
    "- Trained MultinomialNB and SVC models on the training data.\n",
    "\n",
    "4 Model Comparison:\n",
    "\n",
    "- Evaluated the models using accuracy and F1-score.\n",
    "\n",
    "- Provided classification reports for a detailed evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df5d0-cf6c-4d65-a036-c8041f623f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
